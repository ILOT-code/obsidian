
系统提示词、图像、用户问题、已生成的回答这些 token 被一股脑丢给 LLM 来处理。在 LLaVA 模型中，图像 token 的占比非常高。直观的感受是，图像 token 的数量肯定是冗余的，人类在看一张图片时，只会根据问题来专注某一小部分区域，或是专注于图像的某个特点。对视觉 token 的压缩有很大潜力。
下面我总结了一些论文中的方法以及他们发现的现象。

## attention 的不平衡

测试在解码输出的 token 过程中，各类型的 token 获得的 attention 之和以及效率（也就是 attention 的和除以 ）
![[Pasted image 20250320140603.png]]