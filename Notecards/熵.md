
## 1\. 量化信息的量化

　　[信息论](https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA "信息论")是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。信息论的基本想法是一个不太可能的事件发生了，要比一个非常可能的事件发生，能提供更多的信息。

　　在信息论中，我们认为：

-   -   非常可能发生的事件信息量要比较少。在极端情况下，确保能够发生的事件应该没有信息量。
    -   较不可能发生的事件具有更高的信息量。
    -   独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。

　　为了满足上面 3 个性质，定义了一事件 $\mbox{x} = x$ 的**自信息**（self-information）为
$$
\begin{equation}
I(x) = -\log P(x)    \tag{1} 
\end{equation}  
$$   
　　
　　我们使用 $\text{x}$ 表示随机变量，使用 $x_1, x_2,...,x_i,..., x_N$ 或者 $x$ 表示随机变量 $\text{x}$ 可能的取值。当式（1）中 $\log$ 以 2 为底数时，$I(x)$ 单位是比特（bit）或者香农（shannons）；当 $\log$ 以自然常数 $e$ 为底数时，$I(x)$ 单位是奈特（nats）。这两个单位之间可以互相转换。

　　自信息只能处理单个的输出。我们可以使用**香农熵**（Shannon entropy）来对整个概率分布中的不确定性总量进行量化：

$$
\begin{equation}  H(\text{x}) = \mathbb{E}_{\text{x} \sim P}[I(x)] = \sum_{i= 1}^{N} P(x_i)I(x_i) = - \sum_{i= 1}^{N} P(x_i)\log P(x_i)  \tag{2} \end{equation}
$$

式（2）后两个等号是在离散型变量的情况下成立，对于连续型变量，则需要求积分。当 $\text{x}$ 是连续的，香农熵被称为**微分熵**（differential entropy）。


　　**熵的一些性质：**

-   -   那些接近确定性的分布（输出几乎可以确定）具有较低的熵。
    -   那些接近均匀分布的概率分布具有较高的熵。
    - 熵是非负的


## 2\. KL 散度

　　KL 散度全称 Kullback-Leibler (KL) divergence，可以用来衡量两个分布的差异。

　　在概率论与统计中，我们经常会将一个复杂的分布用一个简单的近似分布来代替。KL 散度可以帮助我们测量在选择一个近似分布时丢失的信息量。

　　假设原概率分布为 $P(\text{x})$，近似概率分布为 $Q(\text{x})$，则使用 KL 散度衡量这两个分布的差异：

$$
\begin{equation}  D_{KL}(P||Q) = \mathbb{E}_{\text{x} \sim P}[\log \frac{P(x)}{Q(x)}] = \mathbb{E}_{\text{x} \sim P}[\log P(x) - \log Q(x)]   \tag{3}\end{equation}
$$

　　如果 $\text{x}$ 是离散型变量，式（3）还可以写成如下形式：

$$
\begin{equation} D_{KL}(P||Q) = \sum_{i= 1}^{N} P(x_i) \log\frac{P(x_i)}{Q(x_i)} = \sum_{i= 1}^{N} P(x_i)[\log P(x_i) - \log Q(x_i)]    \tag{4}\end{equation}
$$

　　对于连续型变量，则式（4）不能这么写，需要求积分。如果 $\text{x}$ 是连续型变量，则式（3）中概率分布最好用 $p(\text{x})$和$q(\text{x})$ 代替 $P(\text{x})$和$Q(\text{x})$。习惯上，用小写字母表示连续型变量的概率密度函数（probability density function，PDF），用大写字母表示离散型变量的概率质量函数（probability mass function，PMF）。
>- PDF：概率密度函数（probability density function）, 在数学中，**连续型随机变量**的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。
>- PMF : 概率质量函数（probability mass function), 在概率论中，概率质量函数是**离散随机变量**在各特定取值上的概率。
>- CDF : 累积分布函数 (cumulative distribution function)，又叫分布函数，是**概率密度函数的积分**，能完整描述一个实随机变量X的概率分布。

　　**KL 散度的一些性质：**

-   -   KL 散度是非负的。
    -   KL 散度为 0，当且仅当 $P$ 和 $Q$ 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的。
    -   KL 散度不是真的距离，它不是对称的，即 $D_{KL}(P||Q) \ne D_{KL}(Q||P)$。

## 3\. 交叉熵

　　交叉熵（cross-entropy）和 KL 散度联系很密切。同样地，交叉熵也可以用来衡量两个分布的差异。以离散型变量 $\text{x}$ 为例：

$$
\begin{equation}  H(P, Q) = - \mathbb{E}_{\text{x} \sim P}\log Q(x) = - \sum_{i= 1}^{N} P(x_i) \log Q(x_i) \tag{4} \end{equation}
$$

　　交叉熵 $H(P, Q) = H(P) +  D_{KL}(P||Q)$。其中 $H(P)$（即 $H(\text{x})$ ，其中 $\text{x} \sim P$）为分布 $P$ 的熵，$D_{KL}(P||Q)$ 表示两个分布的 KL 散度。当概率分布 $P(\text{x})$ 确定了时，$H(P)$ 也将被确定，即 $H(P)$ 是一个常数。在这种情况下，交叉熵和 KL 散度就差一个大小为 $H(P)$ 的常数。下面给出一个简单的推导：

　　我们将式（4）中 KL 散度的公式再进行展开：

$$
\begin{equation} \begin{aligned}
D_{KL}(P||Q) &= \sum_{i= 1}^{N} P(x_i)[\log P(x) - \log Q(x)]  \\ &=  \sum_{i= 1}^{N} P(x_i) \log P(x_i)  - \sum_{i= 1}^{N} P(x_i) \log Q(x_i)  \\ &= -[- \sum_{i= 1}^{N} P(x_i) \log P(x_i)]+ [ - \sum_{i= 1}^{N} P(x_i) \log Q(x_i) ] \\ &= - H(P) + H(P, Q) \end{aligned}\end{equation}
$$
　　


即 $H(P, Q) = H(P) +  D_{KL}(P||Q)$。这对连续形变量依旧成立。
　　**交叉熵的一些性质：**

-   -   非负。
    -   和 KL 散度相同，交叉熵也不具备对称性，即 $H(P, Q) \ne H(Q,P)$。
    -   对同一个分布求交叉熵等于对其求熵。

　　
　　为什么既有 KL 散度又有交叉熵？在信息论中，熵的意义是对 $P$ 事件的随机变量编码所需的最小字节数，KL 散度的意义是“额外所需的编码长度”如果我们使用 $Q$ 的编码来表示 $P$，交叉熵指的是当你使用 $Q$ 作为密码来表示 $P$ 是所需要的 “平均的编码长度”。但是在机器学习评价两个分布之间的差异时，由于分布 $P$ 会是给定的，所以此时 KL 散度和交叉熵的作用其实是一样的，而且因为交叉熵少算一项，更加简单，所以选择交叉熵会更好。
