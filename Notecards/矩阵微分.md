[机器学习中的矩阵向量求导](https://www.cnblogs.com/pinard/p/10825264.html)
## 布局

|自变量\因变量|标量 $y$ |向量 $\mathbf {y} \in \mathbb R^m$ |矩阵 $\mathbf Y \in \mathbb R^{p\times q}$ |
|---|---|---|---|
|标量 $x$ |\\| $\frac{\partial{\mathbf y}}{\partial{x}}$ <br>分子布局：$m$ 维列向量（默认布局）<br> 分母布局：$m$ 维行向量| $\frac{\partial{\mathbf Y}}{\partial{ x}}$ <br>分子布局：$p \times q$ 矩阵（默认布局）<br> 分母布局：$q \times p$ 矩阵|
|向量 $\mathbf {x} \in \mathbb R^n$ | $\frac{\partial{y}}{\partial{\mathbf x}}$ <br>分子布局：$n$ 维行向量 <br> 分母布局：$n$ 维列向量（默认布局）| $\frac{\partial{\mathbf y}}{\partial{\mathbf x}}$ <br> 分子布局：$m \times n$ 雅克比矩阵（默认布局）<br> 分母布局：$n \times n$ 梯度矩阵| \\|
|向量 $\mathbf {X} \in \mathbb R^{m\times n}$ | $\frac{\partial{y}}{\partial{\mathbf X}}$ <br> 分子布局：$n \times m$ 矩阵 <br> 分母布局：$m \times n$ 矩阵（默认布局）|\\|\\|




## 标量对向量求导的一些基本法则

1. 线性法则: 如果 $f,g$ 都是实值函数, $c_1,c_2$ 为常数, 则:
   $$
{\frac{\partial(c_{1}f(\mathbf{x})+c_{2}g(\mathbf{x})}{\partial\mathbf{x}}}=c_{1}{\frac{\partial f(\mathbf{x})}{\partial\mathbf{x}}}+c_{2}{\frac{\partial g(\mathbf{x})}{\partial\mathbf{x}}}
$$

2. 乘法法则: 如果 $f,g$ 都是实值函数, 则:
 $$
\frac{\partial f(\mathbf{x})g(\mathbf{x})}{\partial\mathbf{x}}=f(\mathbf{x})\frac{\partial g(\mathbf{x})}{\partial\mathbf{x}}+\frac{\partial f(\mathbf{x})}{\partial\mathbf{x}}g(\mathbf{x})
$$
要注意的是如果不是实值函数, 则不能这么使用乘法法则。



3. 除法法则: 如果 $f,g$ 都是实值函数, 且 $g(\mathbf{x})\neq0$,则:
$$
\frac{\partial f (\mathbf{x})/g (\mathbf{x})}{\partial\mathbf{x}}=\frac{1}{g^{2}(\mathbf{x})}(g (\mathbf{x})\frac{\partial f (\mathbf{x})}{\partial\mathbf{x}}-f (\mathbf{x})\frac{\partial g (\mathbf{x})}{\partial\mathbf{x}})
$$


## 按照定义求导
$y=\mathbf{a}^T\mathbf{X}\mathbf{b}$,求解 $\frac{\partial\mathbf{a}^T\mathbf{X}\mathbf{b}}{\partial\mathbf{X}}$ 。其中, $\mathbf{a}$ 是 $m$ 维向量, $\mathbf{b}$ 是 $n$ 维向量, $\mathbf{X}$ 是 $m\times n$ 的矩阵。我们对矩阵 $\mathbf{X}$ 的任意一个位置的 $\mathbf X_{ij}$。

$$
\frac{\partial\mathbf{a}^T\mathbf{Xb}}{\partial X_{ij}}=\frac{\partial\sum\limits_{p=1}^m\sum\limits_{q=1}^na_pX_{pq}b_q}{\partial X_{ij}}=\frac{\partial a_iX_{ij}b_j}{\partial X_{ij}}=a_ib_j
$$

整理得：

$$
\frac{\partial\mathbf{a}^T\mathbf{X}\mathbf{b}}{\partial\mathbf{X}}=ab^T
$$

$\mathbf{y}=\mathbf{Ax}$,其中 $\mathbf{A}$ 为 $n\times m$ 的矩阵。$\mathbf{x},\mathbf{y}$ 分别为 $m,n$ 维向量。需要求导 $\frac{\partial\mathbf{A}\mathbf{x}}{\partial\mathbf{x}}$，根据定义, 结果应该是一个 $n\times m$ 的矩阵。先求矩阵的第 $i$ 行和向量的内积对向量的第 $j$ 分量求导：
$$
{\frac{\partial\mathbf{A_{i}\mathbf{x}}}{\partial\mathbf{x_{j}}}}={\frac{\partial A_{ij}x_{j}}{\partial\mathbf{x_{j}}}}=A_{ij}
$$

可见矩阵 $\mathbf{A}$ 的第 $i$ 行和向量的内积对向量的第 $j$ 分量求导的结果就是矩阵 $\mathbf{A}$ 的 $(i,j)$ 位置的值。排列起来就是一个矩阵了, 由于我们分子布局, 所以排列出的结果是:
$$
\frac{\partial{\mathbf{Ax}}}{\partial{\mathbf x}}=\mathbf A
$$
## 微分法求导
### 微分法

在高数里面我们学习过标量的导数和微分，他们之间有这样的关系：$df =f'(x)dx$。如果是多变量的情况，则微分可以写成：
$$
df=\sum\limits_{i=1}^n\frac{\partial f}{\partial x_i}dx_i = (\frac{\partial f}{\partial \mathbf{x}})^Td\mathbf{x}
$$

从上次我们可以发现标量对向量的求导和它的向量微分有一个转置的关系。现在我们再推广到矩阵。对于矩阵微分，我们的定义为：
$$
df=\sum\limits_{i=1}^m\sum\limits_{j=1}^n\frac{\partial f}{\partial X_{ij}}dX_{ij} = tr ((\frac{\partial f}{\partial \mathbf{X}})^Td\mathbf{X})
$$
其中第二步使用了矩阵迹的性质，即迹函数等于主对角线的和。即
$$
tr (A^TB) = \sum\limits_{i, j}A_{ij}B_{ij}
$$
从上面矩阵微分的式子，我们可以看到矩阵微分和它的导数也有一个转置的关系，不过在外面套了一个迹函数而已。由于标量的迹函数就是它本身，那么矩阵微分和向量微分可以统一表示，即：
$$
df= tr ((\frac{\partial f}{\partial \mathbf{X}})^Td\mathbf{X})\;\; \; df= tr ((\frac{\partial f}{\partial \mathbf{x}})^Td\mathbf{x})
$$



### 微分的性质

我们在讨论如何使用矩阵微分来求导前，先看看矩阵微分的性质：
1. 微分加减法：$d(X+Y) =dX+dY, d(X-Y) =dX-dY$
2. 微分乘法：$d(XY) =(dX)Y + X(dY)$
3. 微分转置：$d(X^T) =(dX)^T$
4. 微分的迹：$d(tr(X)) =tr(dX)$
5. 微分哈达马乘积： $d(X \odot Y) = X\odot dY + dX \odot Y$
6. 逐元素求导：$d \sigma(X) =\sigma'(X) \odot dX$
7. 逆矩阵微分：$d X^{-1}= -X^{-1}dXX^{-1}$
8. 行列式微分：$d |X|= |X|tr(X^{-1}dX)$　

### 使用微分法

若标量函数 $f$ 是矩阵 $X$ 经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对 $f$ 求微分，再使用迹函数技巧给 $df$ 套上迹并将其它项交换至 $dX$ 左侧, 那么对于迹函数里面在 $dX$ 左边的部分，我们只需要加一个转置就可以得到导数了。

这里需要用到的迹函数的技巧主要有这么几个：
1. 标量的迹等于自己：$tr(x) =x$
2. 转置不变：$tr(A^T) =tr(A)$
3. 交换率：$tr(AB) =tr(BA)$,需要满足 $A,B^T$ 同维度。
4. 加减法：$tr(X+Y) =tr(X)+tr(Y), tr(X-Y) =tr(X)-tr(Y)$
5. 矩阵乘法和迹交换：$tr((A\odot B)^TC)= tr(A^T(B \odot C))$,需要满足 $A,B,C$ 同维度。

**例子一**
$$
y=\mathbf{a}^T\mathbf{X}\mathbf{b}, \frac{\partial y}{\partial \mathbf{X}}
$$

首先，我们使用微分乘法的性质对 $f$ 求微分，得到：
$$
dy = d\mathbf{a}^T\mathbf{X}\mathbf{b} + \mathbf{a}^Td\mathbf{X}\mathbf{b} + \mathbf{a}^T\mathbf{X}d\mathbf{b} = \mathbf{a}^Td\mathbf{X}\mathbf{b}
$$

第二步，就是两边套上迹函数，即：
$$
dy =tr (dy) = tr (\mathbf{a}^Td\mathbf{X}\mathbf{b}) = tr (\mathbf{b}\mathbf{a}^Td\mathbf{X})
$$
根据我们矩阵导数和微分的定义，迹函数里面在 $dX$ 左边的部分 $\mathbf{b}\mathbf{a}^T$,加上一个转置即为我们要求的导数，即：
$$
\frac{\partial f}{\partial \mathbf{X}} = (\mathbf{b}\mathbf{a}^T)^T =ab^T
$$

以上就是微分法的基本流程，先求微分再做迹函数变换，最后得到求导结果。比起定义法，我们现在不需要去对矩阵中的单个标量进行求导了。

**例子二**
$$
y=\mathbf{a}^Texp (\mathbf{X}\mathbf{b}), \frac{\partial y}{\partial \mathbf{X}}
$$

$$
dy =tr (dy) = tr (\mathbf{a}^Tdexp (\mathbf{X}\mathbf{b})) = tr (\mathbf{a}^T (exp (\mathbf{X}\mathbf{b}) \odot d (\mathbf{X}\mathbf{b}))) = tr ((\mathbf{a}  \odot exp (\mathbf{X}\mathbf{b}) )^T d\mathbf{X}\mathbf{b}) =  tr (\mathbf{b}(\mathbf{a}  \odot exp (\mathbf{X}\mathbf{b}) )^T d\mathbf{X})
$$

其中第三步到第 4 步使用了上面迹函数的性质 5. 这样我们的求导结果为：
$$
\frac{\partial y}{\partial \mathbf{X}} =(\mathbf{a}  \odot exp (\mathbf{X}\mathbf{b}) ) b^T
$$
**例子三** 

$\frac{\partial tr(W^TAW)}{\partial W}$: 
$$
d(tr(W^TAW)) = tr(dW^TAW +W^TAdW) = tr(dW^TAW)+tr(W^TAdW) = tr((dW)^TAW) + tr(W^TAdW) = tr(W^TA^TdW) +  tr(W^TAdW) = tr(W^T(A+A^T)dW)
$$

因此可以得到：
$$
\frac{\partial tr (W^TAW)}{\partial W} = (A+A^T) W
$$

**例子四**

$\frac{\partial tr(B^TX^TCXB)}{\partial X}$: 

$$
d (tr (B^TX^TCXB)) = tr (B^TdX^TCXB) + tr (B^TX^TCdXB) = tr ((dX)^TCXBB^T) + tr (BB^TX^TCdX) = tr (BB^TX^TC^TdX) + tr (BB^TX^TCdX) = tr ((BB^TX^TC^T + BB^TX^TC) dX)
$$

因此可以得到
$$
\frac{\partial tr (B^TX^TCXB)}{\partial X}= (C+C^T) XBB^T
$$

## 链式法则

### 向量对向量求导的链式法则

首先我们来看看向量对向量求导的链式法则。假设多个向量存在依赖关系，比如三个向量 $\mathbf{x} \to \mathbf{y} \to \mathbf{z}$ 存在依赖关系，则我们有下面的链式求导法则：
$$
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}}\frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$

该法则也可以推广到更多的向量依赖关系。但是要注意的是要求所有有依赖关系的变量都是向量，如果有一个 $\mathbf{Y}$ 是矩阵，，比如是 $\mathbf{x} \to \mathbf{Y} \to \mathbf{z}$， 则上式并不成立。

从矩阵维度相容的角度也很容易理解上面的链式法则，假设 $\mathbf{x} , \mathbf{y} ,\mathbf{z}$ 分别是 $m,n.p$ 维向量，则求导结果 $\frac{\partial \mathbf{z}}{\partial \mathbf{x}}$ 是一个 $p \times m$ 的雅克比矩阵，而右边 $\frac{\partial \mathbf{z}}{\partial \mathbf{y}}$ 是一个 $p \times n$ 的雅克比矩阵，$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ 是一个 $n \times m$ 的矩阵，两个雅克比矩阵的乘积维度刚好是 $p \times m$，和左边相容。

### 标量对多个向量的链式求导法则

在我们的机器学习算法中，最终要优化的一般是一个标量损失函数，因此最后求导的目标是标量，无法使用上一节的链式求导法则，比如 2 向量，最后到 1 标量的依赖关系：$\mathbf{x} \to \mathbf{y} \to z$，此时很容易发现维度不相容。

假设 $\mathbf{x} , \mathbf{y}$ 分别是 $m,n$ 维向量, 那么 $\frac{\partial z}{\partial \mathbf{x}}$ 的求导结果是一个 $m \times 1$ 的向量, 而 $\frac{\partial z}{\partial \mathbf{y}}$ 是一个 $n \times 1$ 的向量，$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ 是一个 $n \times m$ 的雅克比矩阵, 右边的向量和矩阵是没法直接乘的。

但是假如我们把标量求导的部分都做一个转置，那么维度就可以相容了，也就是：
$$
(\frac{\partial z}{\partial \mathbf{x}})^T = (\frac{\partial z}{\partial \mathbf{y}})^T\frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$

但是毕竟我们要求导的是 $(\frac{\partial z}{\partial \mathbf{x}})$,而不是它的转置，因此两边转置我们可以得到标量对多个向量求导的链式法则：
$$
\frac{\partial z}{\partial \mathbf{x}} = (\frac{\partial \mathbf{y}}{\partial \mathbf{x}} )^T\frac{\partial z}{\partial \mathbf{y}}
$$
如果是标量对更多的向量求导, 比如 $\mathbf{y_1} \to \mathbf{y_2}  \to ...\to  \mathbf{y_n} \to z$，则其链式求导表达式可以表示为：
$$
\frac{\partial z}{\partial \mathbf{y_1}} = (\frac{\partial \mathbf{y_n}}{\partial \mathbf{y_{n-1}}} \frac{\partial \mathbf{y_{n-1}}}{\partial \mathbf{y_{n-2}}} ...\frac{\partial \mathbf{y_2}}{\partial \mathbf{y_1}})^T\frac{\partial z}{\partial \mathbf{y_n}}
$$

这里我们给一个最常见的最小二乘法求导的例子。最小二乘法优化的目标是最小化如下损失函数:
$$
l=(X\theta - y)^T (X\theta - y)
$$

我们优化的损失函数 $l$ 是一个标量，而模型参数 $\theta$ 是一个向量，期望 $l$ 对 $\theta$ 求导，并求出导数等于 0 时候的极值点。我们假设向量 $z = X\theta - y$, 则 $l=z^Tz$， $\theta \to z \to l$ 存在链式求导的关系，因此：
$$
\frac{\partial l}{\partial \mathbf{\theta}} = (\frac{\partial z}{\partial \theta} )^T\frac{\partial l}{\partial \mathbf{z}} = X^T (2 z) =2 X^T (X\theta - y)
$$

其中最后一步转换使用了如下求导公式：
$$
\frac{\partial X\theta - y}{\partial \theta} = X
$$ 
$$
\frac{\partial z^Tz}{\partial z} = 2 z
$$


### 标量对多个矩阵的链式求导法则

下面我们再来看看标量对多个矩阵的链式求导法则，假设有这样的依赖关系：$\mathbf{X} \to \mathbf{Y} \to z$,那么我们有：
$$
\frac{\partial z}{\partial x_{ij}} = \sum\limits_{k, l}\frac{\partial z}{\partial Y_{kl}} \frac{\partial Y_{kl}}{\partial X_{ij}} =tr ((\frac{\partial z}{\partial Y})^T\frac{\partial Y}{\partial X_{ij}})
$$

这里大家会发现我们没有给出基于矩阵整体的链式求导法则，主要原因是矩阵对矩阵的求导是比较复杂的定义，我们目前也未涉及。因此只能给出对矩阵中一个标量的链式求导方法。这个方法并不实用，因为我们并不想每次都基于定义法来求导最后再去排列求导结果。

虽然我们没有全局的标量对矩阵的链式求导法则，但是对于一些线性关系的链式求导，我们还是可以得到一些有用的结论的。

我们来看这个常见问题：$A,X,B,Y$ 都是矩阵，$z$ 是标量，其中 $z= f(Y), Y=AX+B$,我们要求出 $\frac{\partial z}{\partial X}$,这个问题在机器学习中是很常见的。此时，我们并不能直接整体使用矩阵的链式求导法则，因为矩阵对矩阵的求导结果不好处理。

这里我们回归初心，使用定义法试一试, 先使用上面的标量链式求导公式：
$$
\frac{\partial z}{\partial x_{ij}}  = \sum\limits_{k, l}\frac{\partial z}{\partial Y_{kl}} \frac{\partial Y_{kl}}{\partial X_{ij}}
$$

我们再来看看后半部分的导数：
$$
\frac{\partial Y_{kl}}{\partial X_{ij}} =  \frac{\partial \sum\limits_s (A_{ks}X_{sl})}{\partial X_{ij}} =   \frac{\partial A_{ki}X_{il}}{\partial X_{ij}} =A_{ki}\delta_{lj}
$$

其中 $\delta_{lj}$ 在 $l=j$ 时为 1，否则为0.

那么最终的标签链式求导公式转化为：
$$
\frac{\partial z}{\partial x_{ij}}  = \sum\limits_{k, l}\frac{\partial z}{\partial Y_{kl}} A_{ki}\delta_{lj} =  \sum\limits_{k}\frac{\partial z}{\partial Y_{kj}} A_{ki}
$$

即矩阵 $A^T$ 的第 i 行和 $\frac{\partial z}{\partial Y}$ 的第 j 列的内积。排列成矩阵即为：
$$
\frac{\partial z}{\partial X} = A^T\frac{\partial z}{\partial Y}
$$

总结下就是：
$$
z= f (Y), Y=AX+B \to \frac{\partial z}{\partial X} = A^T\frac{\partial z}{\partial Y}
$$

这结论在 $\mathbf{x}$ 是一个向量的时候也成立，即：
$$
z= f (\mathbf{y}), \mathbf{y}=A\mathbf{x}+\mathbf{b} \to \frac{\partial z}{\partial \mathbf{x}} = A^T\frac{\partial z}{\partial \mathbf{y}}
$$

如果要求导的自变量在左边，线性变换在右边，也有类似稍有不同的结论如下，证明方法是类似的，这里直接给出结论：
$$
z= f (Y), Y=XA+B \to \frac{\partial z}{\partial X} = \frac{\partial z}{\partial Y}A^T
$$

$$
z= f (\mathbf{y}), \mathbf{y}=X\mathbf{a}+\mathbf{b} \to \frac{\partial z}{\partial \mathbf{X}} = \frac{\partial z}{\partial \mathbf{y}}a^T
$$
使用好上述四个结论，对于机器学习尤其是深度学习里的求导问题可以非常快的解决。
