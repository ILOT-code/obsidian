---
zotero-key: IW 2 FDBIC
zt-attachments:
  - "145"
title: "COINR: COMPRESSED IMPLICIT NEURAL REPRESENTATIONS"
authors: 
doi: 10.48550/arXiv. 2402.097
conference: ICLR 2025 Anonymous
citekey: 
tags:
---

## Abstract
在压缩任务中，INR 方法需要对参数进行压缩，本文聚焦对模型参数的压缩手段。
COINR利用 INR 权重向量空间中的模式，使用字典中的稀疏的查询向量来压缩这些向量空间，并且字典本身不需要传递。
并且，这种方法可以嵌入到任何 INR 方法中，与后续的量化和熵编码也是相容的。


## Method
压缩感知理论认为，现实世界的各种信号经过某种转变之后，会呈现出稀疏性。INR 是对信号进行建模的方法，可以假设 MLP 的参数同样会呈现出这样的稀疏性。

为了寻找这样的稀疏性，首先可以采用正则化和减枝的方法。前者促进参数的稀疏化，后者则强迫参数出现某种程度的稀疏性。作者称，在压缩任务中，面对某些数据模式时，这两种方法都不可避免出现了性能的下降，并且只有小部分的剪纸才能达到满意的结果，压缩效果不好。

另一种方法则是探究 INR 压缩中，权重内在的规律和模式。作者最开始从降维的角度看待这个问题，但降维之后的权重并未呈现某种规律，甚至面对不同的图像都会呈现不一样的模式。INR 权重本身就呈现出了很强的特征：它们很好的符合正太分布。


假设某个隐藏层有 $k_{1}$ 个神经元，那么该层的一个权重向量可以写成 $\mathbf{w}\in \mathbb{R}^{k_{1}}$。它们可以看成一个个符合正态分布的随机变量。
![[Pasted image 20241112144950.png]]

因此，可以用足够多的随机变量的线性表达，来表示该权重向量，