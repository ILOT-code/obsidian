
## 2021NIPS, Deep Contextual Video Compression

视频编码技术，如hevc 等，都是使用预测编码技术。首先预测出当前帧 $\hat{x}_{t}$，再编码一个差值 $x_{t}-\hat{x}_{t}$.编码长度是 $H(x_{t}-\hat{x}_{t})$。显然地，$H(x_{t}-\hat{x}_{t})\geq H(x_{t}|\hat{x}_{t}))$。理论与工程是存在鸿沟的，如果没有深度学习技术的话，直接从预测帧中提取信息来编码当前帧是十分困难的，不如退而求其次，只考虑这一误差项的编码。在预测技术固定的情况下，误差项在统计上会符合某种概率分布，对其编码易于操作。

深度学习技术给了我们探究 $H(x_{t}|\hat{x}_{t})$ 极限的可能。这样的压缩范式称为：条件编码方案。这种编码方案需要解决三个问题：
1. 什么是条件
2. 怎么使用条件
3. 如何学习条件
DCVC 整个系列都在研究这三个问题。

### 架构
![[Pasted image 20250101144912.png]]
#### 条件是什么？
特征信息来自当前帧和上一个解码帧，经加强模块后得到。
#### 怎么使用条件？
编码端，解码端，熵模型同时使用了条件。
#### 如何学习条件？
视频的相邻帧具有很强的相关性，上一解码帧提供了这样的参考。同时，当前帧和上一帧又存在着不同，为此使用 mv 来显式地扭曲该信息。这样的对齐降低了学习的难度。

### 熵模型
![[Pasted image 20250101150703.png]]

采用 AR 的形式，综合融合了三种信息。spatial correlation 引入了非常局部的参考信息，边信息引入了当前帧的全局参考信息，context 则引入了时间轴上的参考信息。

## Temporal Context Mining for Learned  Video Compressionz
在 DCVC 中，在时间轴上，实际上只考虑了上一个时刻的参考，这在视频编码中是次优的，在时间轴上寻找更多的参考有益于视频编码, 这同时给条件的学习带来了困难。
本文的改进点在于从时间轴上获取更多信息，并把条件设计成多尺度的形式以减小学习的难度。

### 架构
![[Pasted image 20250101154446.png]]

#### 条件是什么
多通道的特征 $F$ 被传播与更新，TCM 模块产生的多尺度上下文被编解码端和熵模型使用。mv 信息同样被用来扭曲 $F$
#### 怎么使用条件
编解码器：
![[Pasted image 20250101155023.png]]
编解码器是对称结构，多尺度的条件分别在不同的阶段被融入


熵模型：
![[Pasted image 20250101155056.png]]
多尺度的条件经过融合后被输入到熵模型中。本文的熵模型放弃了 AR。

#### 如何学习条件
![[Pasted image 20250101155418.png]]
$F,v$ 经过下采样后进行扭曲，大尺度的条件融合了小尺度的条件。

## Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression
视频压缩使用的熵模型大多使用现有的图像压缩中的模型，它们的一个缺点是没有考虑到时间轴上的参考。本文为视频压缩方案设计了一种新的熵模型，来缓和熵模型对时间参考有限的问题。

该熵模型完全建立在 TCM 的基础上。

### 时空信息融合的熵模型：
![[Pasted image 20250101162653.png]]

空间信息上的参考实际上采用了 checkboard 的形式，而空间信息上的参考实际上就是融合了 $\hat{y}_{t-1}$ 和 $C_{t}$。$\hat{y}_{y-1}$ 和 $C_{t}$ 是互补的关系，前者在 latent 域上提供了更多参考，后者提供了更多帧间运动的信息。熵模型还会为每个点生成一个量化信息，根据视频内容自适应调整量化值。

### 单模型实现码率调整
码率调整通常通过调整损失函数的 $\lambda$ 来进行，这很不方便。对 latent, 设计了新的量化方式：
![[Pasted image 20250101163936.png]]
global 由用户指定，提供了粗略的量化范围，在模型训练时，global 采样生成。
ch 是一组可学习的参数，分通道进行量化。
sc 是熵模型生成的 spatial-channel-wise 的量化参数。


## Neural Video Compression with Diverse Contexts
上下文的质量极大影响了编码效率。之前的文献设计上下文的想法都很好，但实际的效果很可能不如原先的想法。比如提取光流的模块就很可能根本无法生成正确的光流，因为解码的帧 $\hat{x}_{t-1}$ 具有一定的失真，这种失真甚至会逐渐增加。对特征进行对齐时，直接使用该光流进行对齐，也可能不太恰当。

人为引入的先验知识，能够帮助模型更好的学习。本文分析了之前模型的问题，并设计了相应的解决方法。

### 架构
![[Pasted image 20250101184806.png]]
基于 HEM 进行改进。

### Hierarchical Quality Structure
传统编码器存在着 I, P, B 帧的概念，这种概念最早是为了控制码率，但也提高了编码效率。周期性的提高解码的质量可以生成更准确的上下文，比如说光流 mv，这有效减轻了误差的传播。

HEM 框架中，含有 QP 这样的参数，可以周期性的降低 QP 的值，来提高帧的质量。但作者认为，深度学习对训练分布之外的其他分布不具有很好的鲁棒性，自定义的 QP 层可能无法适应测试集上的分布。

作者首先进行对视频进行分组，为一组中的帧设置不同的损失项权重：
$$
L_{all}=\frac{1}{T}\sum_{t}^{T}(\boldsymbol{w}_{t}\cdot\lambda\cdot dist(x_{t}-\hat{x}_{t})+r_{t})
$$
这种设置可以促使编码器周期性的生成高质量的帧，这样高质量帧的特征会随着 $F$ 进行传递，带来性能增益。

### Group-Based Offset Diversity
$F_{t-1}$ 包含了很复杂的特征，包含了之前所有帧中的信息，而直接使用 $x_{t},\hat{x}_{t-1}$ 之间的光流去对齐它则显得不那么有说服力。之前的研究显示，可变形的对齐方案（引入多样的偏置选择）为图像复原带来了更好的结果。作者为视频压缩设计了类似的方案。

首先在通道维度上把 $F$ 分为 G 组，需要为每一组生成 N 个，共 $G\times N$ 个光流。这些生成的光流以原始的那个光流为基础。Offset Prediction 模块同时会为每一个光流生成置信度 mask。
![[Pasted image 20250101191548.png]]

Cross-group 模块先进行 warp 和 mask 操作，然后，像 PixelShuffle 那样，把这些通道进行重排，顺序变为了 ($N\times G$)，随后的 fusion 把每 N 个通道融合为一个。这样的操作加强了通道间的联系，加强了上下文的丰富性。
![[Pasted image 20250101191717.png]]

### Quadtree Partition-Based Entropy Coding
在熵模型方面，作者设计了四分树的模式来引入更多空间上的参考。

首先把 latent 沿着通道维度分为 4 组，每组内部进行顺序不同的计算策略。每一个阶段解码得到的值会作为参考供下一个阶段使用。


![[Pasted image 20250101192420.png]]

## Neural Video Compression with Feature Modulation
本文是为了解决 NVC 领域的几个问题：
1. 如何在单个模型中，允许一个宽的解码质量范围。
2. 在长预测链之下，NVC 如何才能稳定工作。现有的一些 NVC 方法很难处理好时间误差累积问题，它们很多通过频繁地插入 I-帧来解决这样的问题，但很多情况下对于单个视频，是只允许存在 1 个 I-帧的。
### Wide Quality Range in a Single Model
![[Pasted image 20250101194448.png]]
首先，有一个质量区间 $[0,q_{num}-1]$，用户可以选择一个此区间的数 $q$ 来决定帧的质量。

编解码被分为了两段过程，高分辨率和低分辨率，$s_t^{enc}$ 和 $s_t^{dec}$ 分别被用来调制和解调两者中间的 latent。$s_t^{enc}$ 和 $s_t^{dec}$ 类似于 $\hat{I}=QS\cdot\lfloor\frac I{QS}\rceil$ 中的 $QS$，只是没有限定这个量化参数在编解码过程中需要相同，作者认为这样能够更加灵活地调整图像质量。也因此，量化过程并没有使用除法，而是使用乘法运算。
$s_t^{enc}$ 由两个可学习的参数 $s_{min}^{enc},s_{max}^{enc}$ 和自定义的 $q$ 来生成：
$$
\begin{align*}
s_t^{enc}=s_{min}^{enc}\cdot(\frac{s_{max}^{enc}}{s_{min}^{enc}})^{\frac{q_t}{q_{num}-1}}\\
s_t^{enc}=e^{\ln s_{min}^{enc}+\frac{q_t}{q_{num}-1}\cdot(\ln s_{max}^{enc}-\ln s_{min}^{enc})}
\end{align*}
$$
同时，$Loss_{RD}=R+\lambda D$ 中的 $\lambda$ 也由 $q$ 来决定：
$$
\lambda=e^{\ln\lambda_{min}+\frac{q_t}{q-num-1}\cdot(\ln\lambda_{max}-\ln\lambda_{min})}
$$
$\lambda_{max},\lambda_{min}$ 是预先就固定的数。
在实际训练过程中，会随机在 $[0,q_{num}-1]$ 取样得到 $q$，进行实验。$q$ 越大，$\lambda$ 越大，图像质量占比就越大，就会指导 $s_t^{enc}$ 的生成越趋于 1 的数。通过调整 $\lambda$ 的范围，就能实现很宽的图像质量范围。

$s_t^{enc}$ 对所有空间坐标的点都是一致的，这会导致忽略一些空间特征。为此，使用熵模型生成 spatial-channel-wise 的量化参数 $w_t^{enc}$, 它不仅有助于在每个位置实现精确调制，而且可以适应每个帧的视频内容.

通过给不同帧不同的 $q$，可以实现质量的控制：
![[Pasted image 20240609135454.png]]

### Long Prediction Chain
即使 DCVC-DC 引入分层质量结构来周期性地提高帧的质量，以减轻时间特征误差累积问题，当仅使用单个 I-帧时，问题依旧严重。这是因为在长链条下，$F$ 中包含了一些无用的信息。

第一个改进是在训练期间增加视频帧数。虽然是一个简单的修改，但是还是很有帮助的。较长的视频可以识别长距离内的相似模式，然后更好地探索时间相关性。
第二个改进是对时间特征 $F$ 进行调制，定期更新它，以去除累积误差和不相关信息。
在新周期开始时，使用 FE 从 $\hat{x}_{t-1}$ 中提取时间特征，替换原来的 $F_{t-1}$，进行传递。
![[Pasted image 20240609141247.png]]
![[Pasted image 20240609140704.png]]
