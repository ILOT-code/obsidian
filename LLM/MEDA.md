---
title: "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference"
conference: NAACL 2025
---
# Abstract
之前看过一些针对单语言模态的 kv-cache 汰换策略，MEDA 则为多模态模型引入 kv-cache 汰换策略。