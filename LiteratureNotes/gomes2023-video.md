---
zotero-key: 2ES2TF82
zt-attachments:
  - "356"
title: Video Compression With Entropy-Constrained Neural Representations
authors:
  - Carlos Gomes
  - Roberto Azevedo
  - Christopher Schroers
conference: CVPR2023
citekey: gomes2023-video
tags:
---
# Video Compression With Entropy-Constrained Neural Representations

**æ–‡ç« é“¾æŽ¥**: [Zotero](zotero://select/library/items/2ES2TF82) [attachment](<file:///home/ilot/Zotero/storage/SUVWIVMH/Gomes%20%E7%AD%89%20-%202023%20-%20Video%20Compression%20With%20Entropy-Constrained%20Neural%20.pdf>)
**ç½‘é¡µé“¾æŽ¥**: [URL](https://openaccess.thecvf.com/content/CVPR2023/html/Gomes_Video_Compression_With_Entropy-Constrained_Neural_Representations_CVPR_2023_paper.html)
## Abstract

>[!abstract]
>neural video representation å’Œä¼ ç»Ÿçš„ä¸€äº›è§†é¢‘åŽ‹ç¼©æŠ€æœ¯ä»ç„¶å­˜åœ¨ gapï¼Œè¿™æœ‰ä¸¤æ–¹é¢çš„å› ç´ ï¼š
>1. æ­¤ç±»æ–¹æ³•çš„ç½‘ç»œç»“æž„å¾ˆç®€å•ï¼Œæ— æ³•èŽ·å¾—ç´§å‡‘çš„è¡¨è¾¾å½¢å¼ï¼Œå‚æ•°çš„ä½¿ç”¨å¾ˆæ²¡æœ‰æ•ˆçŽ‡
>2. å¤±çœŸæŽ§åˆ¶å’Œç çŽ‡æŽ§åˆ¶æ²¡æœ‰åŒæ­¥è¿›è¡Œ
>ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å·ç§¯ç»“æž„çš„è§†é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿè”åˆä¼˜åŒ–çŽ‡-å¤±çœŸã€‚


## Architecture
æŠŠå½“å‰å¸§ $t \in[0,1)$ æ‰©å±•åˆ° $T\in \mathbb{R}^{1\times h\times w}$ï¼Œå¹¶æŠŠå®ƒå’Œä¸€ä¸ªå›ºå®šçš„åæ ‡ç½‘æ ¼ $M\in \mathbb{R}^{2\times h\times w},whereM[0,h^{\prime},w^{\prime}]=\frac{w^{\prime}}{W},M[1,h^{\prime},w^{\prime}]=\frac{h^{\prime}}{W}$ ç»“åˆã€‚å† element-wise ä½¿ç”¨ä½ç½®ç¼–ç ï¼š

$$
\gamma(x)=(\sin(1.25^0\pi x),\cos(1.25^0\pi x),\dots,\sin(1.25^{L-1}\pi x,\cos(1.25^{L-1}\pi x))
$$
é‡‡ç”¨äº†å’Œ E-NERV ç±»ä¼¼çš„ç»“æž„ã€‚å·ç§¯ä¸Šé‡‡æ ·å…±æœ‰ 5 ä¸ªå—ï¼ŒAdaIN æ¨¡å—ä¸ºå®ƒä»¬å„å­¦ä¹ ä¸€ä¸ª $\mu ï¼Œ\sigma$ï¼Œæ—¶é—´ä¿¡æ¯ç» AdaIN å¤„ç†åŽè¾“å…¥è¿™äº›å±‚ã€‚
![[Pasted image 20240906111014.png]]

## Entropy Minimization
æƒé‡å‚æ•°éœ€è¦è¢«é‡åŒ–ï¼Œ$\hat{\theta}=Q_{\gamma}(\theta)$ï¼Œè¢«è§£ç åŽï¼Œå†åé‡åŒ–å›žæ¥ $\theta ^\prime=Q_{\gamma}^{-1}(\hat{\theta})$. æ•´ä½“çš„æŸå¤±å‡½æ•°å³ä¸ºï¼š
$$
\min_{\theta}\sum_{(x,y)\in S}D(f(x;Q_{\gamma}^{-1}(\hat{\theta})),y)+\lambda \sum_{i=0}^{|\hat{\theta}|}-\log_{2}\hat{p}(\hat{\theta_{i}})
$$
ä½¿ç”¨çš„é‡åŒ–æ“ä½œæ˜¯ï¼š
$$
\begin{aligned}
Q_{\gamma}:\mathbb{R}\to \mathbb{Z},Q_{\gamma}(x)=\lfloor \frac{x+\beta}{\alpha} \rceil , \gamma=(\alpha,\beta)\\
Q_{\gamma}^{-1}=x\times \alpha-\beta
\end{aligned} 
$$

å…‹æœä¸å¯å¾®çš„æ–¹æ³•æœ‰ä¸¤ç§ï¼š
1. STE
2. åŠ å™ªå£°ï¼Œ$\tilde{\theta}=\frac{x+\beta}{\alpha}+u$

å®žé™…ä¸Šä½œè€…åŒæ—¶ä½¿ç”¨äº†è¿™ä¸¤ç§æ–¹æ³•ã€‚åœ¨è®¡ç®—æŸå¤±å‡½æ•°çš„ç¬¬ä¸€é¡¹æ—¶ï¼Œå¯¹å‚æ•°é¦–å…ˆä½¿ç”¨äº† STE è¿›è¡Œå¤„ç†ï¼Œè®¡ç®—å¤±çœŸã€‚åœ¨è®¡ç®—ç¬¬äºŒé¡¹æ—¶ï¼Œæ˜¯å¯¹å‚æ•°è¿›è¡ŒåŠ å™ªå£°çš„å¤„ç†ï¼Œå†ä½¿ç”¨ç†µæ¨¡åž‹æ¥è®¡ç®— rateã€‚

ä½œè€…æŠŠæ¯ä¸€å±‚å†…éƒ¨çš„å‚æ•°ï¼Œå»ºæ¨¡æˆç‹¬ç«‹åŒåˆ†å¸ƒæ¨¡åž‹ã€‚ï¼ˆä½œè€…å£°ç§°é‡‡ç”¨è¿‡è”åˆæ¦‚çŽ‡å»ºæ¨¡çš„æ–¹æ³•ï¼Œä½†æ•ˆæžœä¸å¥½ï¼‰ã€‚å¹¶ä½¿ç”¨ [[balle2018-variational|(Johannes BallÃ©, 2018)]] ä¸­é‚£ä¸ªå¯¹ $z$ è¿›è¡Œæ¦‚çŽ‡å»ºæ¨¡çš„æ¨¡åž‹ã€‚

æœ€ç»ˆæŸå¤±å‡½æ•°å¯ä»¥å†™ä¸ºï¼š
$$
\min _{\theta,\phi,\boldsymbol{\gamma}}\sum_{(x,y)\in S}D(f(x;Q^{-1}(Q_{ste}(\theta;\boldsymbol{\gamma});\boldsymbol{\gamma})),y)+\lambda\frac{\sum_{l\in layers}-\log_{2}p_{\phi}(Q_{noise}(\theta _{l};\gamma_{l}))}{frames\times h\times w}
$$
rate é¡¹é™¤ä»¥äº†æ€»åƒç´ æ•°ï¼Œè¿™æ · $\lambda$ å°±å’Œè§†é¢‘çš„åˆ†è¾¨çŽ‡å’Œå¸§æ•°æ— å…³äº†ã€‚


## Experiments

### å®¹é‡æµ‹è¯•
ä¸ºéªŒè¯è¿™ä¸ªæž¶æž„çš„å®¹é‡è€Œä½œçš„å®žéªŒï¼Œå› æ­¤ä¸ä½¿ç”¨ç†µæ¨¡åž‹å’Œé‡åŒ–ã€‚
![[Pasted image 20240906121016.png|500]]
### è§†é¢‘åŽ‹ç¼©
![[Pasted image 20240906121336.png]]

### å¾®è°ƒ
æŠŠç†µä¼˜åŒ–çš„è¿‡ç¨‹çœ‹æˆå¾®è°ƒã€‚å‰ $0.8epoch$ è¿‡ç¨‹ä¸­ï¼Œ$\lambda=0$ï¼ŒåŽ $0.2epoch$ æ‰ä½¿ç”¨çœŸæ­£çš„ $\lambda$ã€‚
å®žéªŒå‘çŽ°è¿™ç§ç­–ç•¥åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ï¼Œæ€§èƒ½æŸå¤±å¾ˆå°ã€‚


## Comments

### å…ˆå‰ç ”ç©¶çš„é—®é¢˜

> [!note] Page 1
> 
> This performance gap can be explained by the fact that current NVR methods: i) use architectures that do not efficiently obtain a compact representation of temporal and spatial information; and ii) minimize rate and distortion disjointly (first overfitting a network on a video and then using heuristic techniques such as post-training quantization or weight pruning to compress the model).
> 
> ---
> ðŸ”¤è¿™ç§æ€§èƒ½å·®è·å¯ä»¥é€šè¿‡ä»¥ä¸‹äº‹å®žæ¥è§£é‡Šï¼šå½“å‰çš„ NVR æ–¹æ³•ï¼š i) ä½¿ç”¨çš„æž¶æž„ä¸èƒ½æœ‰æ•ˆåœ°èŽ·å¾—æ—¶é—´å’Œç©ºé—´ä¿¡æ¯çš„ç´§å‡‘è¡¨ç¤ºï¼› ii) ä¸ç›¸äº¤åœ°æœ€å°åŒ–é€ŸçŽ‡å’Œå¤±çœŸï¼ˆé¦–å…ˆåœ¨è§†é¢‘ä¸Šè¿‡åº¦æ‹Ÿåˆç½‘ç»œï¼Œç„¶åŽä½¿ç”¨å¯å‘å¼æŠ€æœ¯ï¼ˆä¾‹å¦‚è®­ç»ƒåŽé‡åŒ–æˆ–æƒé‡ä¿®å‰ªï¼‰æ¥åŽ‹ç¼©æ¨¡åž‹ï¼‰ã€‚ðŸ”¤
> ^NH5XTGX4aSUVWIVMHp1

> [!note] Page 2
> 
> Previous efforts in using INRs for video compression, however, have mostly treated the problems of representing the input signal with high fidelity and compressing it as mostly disjoint tasks. A network is first trained to minimize a distortion loss and is then put through some procedure to reduce its size, e.g., storing its weights in a 16-bit floating point format, quantization, or pruning [7, 10, 29]. Furthermore, current architectures are not designed with parameter efficiency as a priority, suffering from an inefficient allocation of parameters, which can be prohibitive for the task of video compression [7, 16].
> 
> ---
> ðŸ”¤ç„¶è€Œï¼Œä¹‹å‰ä½¿ç”¨ INR è¿›è¡Œè§†é¢‘åŽ‹ç¼©çš„åŠªåŠ›å¤§å¤šå°†é«˜ä¿çœŸåº¦è¡¨ç¤ºè¾“å…¥ä¿¡å·å¹¶å°†å…¶åŽ‹ç¼©ä¸ºå¤§å¤šæ•°ä¸ç›¸äº¤ä»»åŠ¡çš„é—®é¢˜æ¥å¤„ç†ã€‚é¦–å…ˆè®­ç»ƒç½‘ç»œä»¥æœ€å°åŒ–å¤±çœŸæŸå¤±ï¼Œç„¶åŽé€šè¿‡ä¸€äº›ç¨‹åºæ¥å‡å°å…¶å¤§å°ï¼Œä¾‹å¦‚ï¼Œä»¥ 16 ä½æµ®ç‚¹æ ¼å¼å­˜å‚¨å…¶æƒé‡ã€é‡åŒ–æˆ–ä¿®å‰ª [7,10,29]ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æž¶æž„åœ¨è®¾è®¡æ—¶å¹¶æœªå°†å‚æ•°æ•ˆçŽ‡ä½œä¸ºä¼˜å…ˆè€ƒè™‘å› ç´ ï¼Œå¯¼è‡´å‚æ•°åˆ†é…æ•ˆçŽ‡ä½Žä¸‹ï¼Œè¿™å¯èƒ½æ— æ³•å®Œæˆè§†é¢‘åŽ‹ç¼©ä»»åŠ¡[7, 16]ã€‚ðŸ”¤
> ^H73CDBESaSUVWIVMHp2

### åˆ›æ–°ç‚¹

> [!note] Page 1
> 
> We propose a novel convolutional architecture for video representation that better represents spatio-temporal information and a training strategy capable of jointly optimizing rate and distortion.
> 
> ---
> ðŸ”¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘è¡¨ç¤ºå·ç§¯æž¶æž„ï¼Œå¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºæ—¶ç©ºä¿¡æ¯ï¼Œä»¥åŠèƒ½å¤Ÿè”åˆä¼˜åŒ–é€ŸçŽ‡å’Œå¤±çœŸçš„è®­ç»ƒç­–ç•¥ã€‚ðŸ”¤
> ^S7HQ88SPaSUVWIVMHp1

> [!note] Page 2
> 
> In addition, drawing on information theory, we propose a theoretically motivated R-D formulation for video compression with INRs that jointly minimizes rate and distortion.
> 
> ---
> ðŸ”¤æ­¤å¤–ï¼Œåˆ©ç”¨ä¿¡æ¯è®ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºŽç†è®ºçš„ R-D è§†é¢‘åŽ‹ç¼©å…¬å¼ï¼Œå…¶ INR å¯ä»¥å…±åŒæœ€å°åŒ–é€ŸçŽ‡å’Œå¤±çœŸã€‚ðŸ”¤
> ^8B7RS4QMaSUVWIVMHp2

> [!note] Page 3
> 
> a more efficient neural network architecture for video compression (Section 3.1) and the formalization of the task as an R-D problem (Section 3.2).
> 
> ---
> ðŸ”¤ç”¨äºŽè§†é¢‘åŽ‹ç¼©çš„æ›´é«˜æ•ˆçš„ç¥žç»ç½‘ç»œæž¶æž„ï¼ˆç¬¬ 3.1 èŠ‚ï¼‰ä»¥åŠå°†ä»»åŠ¡å½¢å¼åŒ–ä¸º R-D é—®é¢˜ï¼ˆç¬¬ 3.2 èŠ‚ï¼‰ã€‚ðŸ”¤
> ^DTR3I2EZaSUVWIVMHp3

### æ€§èƒ½æˆç»©

> [!note] Page 1
> 
> Moreover, we deliver the first NVR-based video compression method that improves over the typically adopted HEVC benchmark (x265, disabled b-frames, â€œmediumâ€ preset), closing the gap to autoencoder-based video compression techniques.
> 
> ---
> ðŸ”¤æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ç¬¬ä¸€ä¸ªåŸºäºŽ NVR çš„è§†é¢‘åŽ‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¯”é€šå¸¸é‡‡ç”¨çš„ HEVC åŸºå‡†ï¼ˆx265ã€ç¦ç”¨ B å¸§ã€â€œä¸­â€é¢„è®¾ï¼‰æœ‰æ‰€æ”¹è¿›ï¼Œç¼©å°äº†ä¸ŽåŸºäºŽè‡ªåŠ¨ç¼–ç å™¨çš„è§†é¢‘åŽ‹ç¼©æŠ€æœ¯çš„å·®è·ã€‚ðŸ”¤
> ^N72RRVAUaSUVWIVMHp1

### æŠ€æœ¯ç»†èŠ‚

> [!note] Page 2
> 
> We build on the work of Oktay et al.
> 
> ---
> ðŸ”¤æˆ‘ä»¬ä»¥ Oktay ç­‰äººçš„å·¥ä½œä¸ºåŸºç¡€ã€‚ðŸ”¤
> ^VXP7LCAVaSUVWIVMHp2

> [!note] Page 5
> 
> To fit this paradigm, we define: i) a quantization function QÎ³, with learnable parameters Î³, mapping continuous weights to discrete symbols, and ii) a dequantization function Qâˆ’1 Î³ , mapping the symbols to the values at the center of their respective quantization bins.
> 
> ---
> ðŸ”¤ä¸ºäº†é€‚åº”è¿™ç§èŒƒå¼ï¼Œæˆ‘ä»¬å®šä¹‰ï¼šiï¼‰é‡åŒ–å‡½æ•° QÎ³ï¼Œå…·æœ‰å¯å­¦ä¹ å‚æ•° Î³ï¼Œå°†è¿žç»­æƒé‡æ˜ å°„åˆ°ç¦»æ•£ç¬¦å·ï¼Œä»¥åŠ iiï¼‰åé‡åŒ–å‡½æ•° Qâˆ’1 Î³ï¼Œå°†ç¬¦å·æ˜ å°„åˆ°å…¶å„è‡ªä¸­å¿ƒçš„å€¼é‡åŒ–ä»“ã€‚ðŸ”¤
> ^VTSFHHMSaSUVWIVMHp5

> [!note] Page 5
> 
> where Ë† p is the pmf of Ë† Î¸, which can be readily computed. To optimize this loss, the process minimizes the distortion by learning parameters Î¸ that can appropriately represent the signal, and Î³ that provide a small enough quantization error. Simultaneously, the distribution of QÎ³(Î¸) must also have a sufficiently small entropy, to minimize the R.
> 
> ---
> ðŸ”¤å…¶ä¸­ Ë† p æ˜¯ Ë† Î¸ çš„ pmfï¼Œå¾ˆå®¹æ˜“è®¡ç®—ã€‚ä¸ºäº†ä¼˜åŒ–è¿™ç§æŸå¤±ï¼Œè¯¥è¿‡ç¨‹é€šè¿‡å­¦ä¹ å¯ä»¥é€‚å½“è¡¨ç¤ºä¿¡å·çš„å‚æ•° Î¸ å’Œæä¾›è¶³å¤Ÿå°çš„é‡åŒ–è¯¯å·®çš„ Î³ æ¥æœ€å°åŒ–å¤±çœŸã€‚åŒæ—¶ï¼ŒQÎ³(Î¸) çš„åˆ†å¸ƒä¹Ÿå¿…é¡»å…·æœ‰è¶³å¤Ÿå°çš„ç†µï¼Œä»¥æœ€å°åŒ– Rã€‚ðŸ”¤
> ^YJ6875QSaSUVWIVMHp5

> [!note] Page 5
> 
> From the above, we identify the two error sources introduced in this process.
> 
> ---
> ðŸ”¤ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºäº†è¯¥è¿‡ç¨‹ä¸­å¼•å…¥çš„ä¸¤ä¸ªè¯¯å·®æºã€‚ðŸ”¤
> ^82FWZ3ZRaSUVWIVMHp5

> [!note] Page 5
> 
> We define these as two functions, Qnoise and Qste. As in Balle et al. [4], we obtain the best results using Qste for calculating the distortion metric, as it avoids the introduction of random noise, and Qnoise for calculating the entropy term.
> 
> ---
> ðŸ”¤æˆ‘ä»¬å°†å®ƒä»¬å®šä¹‰ä¸ºä¸¤ä¸ªå‡½æ•°ï¼šQnoise å’Œ Qsteã€‚æ­£å¦‚ Balle ç­‰äººæ‰€è¨€ã€‚ [4]ï¼Œæˆ‘ä»¬ä½¿ç”¨ Qste è®¡ç®—å¤±çœŸåº¦é‡èŽ·å¾—äº†æœ€ä½³ç»“æžœï¼Œå› ä¸ºå®ƒé¿å…äº†å¼•å…¥éšæœºå™ªå£°ï¼Œå¹¶ä½¿ç”¨ Qnoise è®¡ç®—ç†µé¡¹ã€‚ðŸ”¤
> ^5Q9EDDHVaSUVWIVMHp5

> [!note] Page 6
> 
> To model the weights of the neural network, we interpret weights in each layer as being generated from an independent source. Within each layer, weights are taken as i.i.d. samples.
> 
> ---
> ðŸ”¤ä¸ºäº†å¯¹ç¥žç»ç½‘ç»œçš„æƒé‡è¿›è¡Œå»ºæ¨¡ï¼Œæˆ‘ä»¬å°†æ¯å±‚ä¸­çš„æƒé‡è§£é‡Šä¸ºä»Žç‹¬ç«‹æºç”Ÿæˆã€‚åœ¨æ¯ä¸€å±‚ä¸­ï¼Œæƒé‡è¢«è§†ä¸ºç‹¬ç«‹åŒåˆ†å¸ƒã€‚æ ·å“ã€‚ðŸ”¤
> ^763NFYKNaSUVWIVMHp6

