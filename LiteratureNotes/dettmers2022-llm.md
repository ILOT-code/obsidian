---
zotero-key: B68PDARC
zt-attachments:
  - "150"
title: "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
authors:
  - Tim Dettmers
  - Mike Lewis
  - Younes Belkada
  - Luke Zettlemoyer
doi: 10.48550/arXiv.2208.07339
conference: xxx
citekey: dettmers2022-llm
tags: []
---
# LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale

**文章链接**: [Zotero](zotero://select/library/items/B68PDARC) [attachment](<file:///home/ilot/Zotero/storage/CCLIH73H/Dettmers%20%E7%AD%89%20-%202022%20-%20LLM.int8()%208-bit%20Matrix%20Multiplication%20for%20Transf.pdf>)
**网页链接**: [URL](http://arxiv.org/abs/2208.07339)
## Abstract

>[!abstract]
>本文为 Transforemt 的前馈层和注意力层设计了一种 INT 8 的矩阵乘法。
>主要使用了两种方法：
>为矩阵乘的每一对内积使用单独的尺度缩放因子，
>对一些数据异常大的特征向量进行全精度计算。

## Introduction

