---
zotero-key: HSTB42UP
zt-attachments:
  - "323"
title: "SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations"
authors:
  - Sharath Girish
  - Abhinav Shrivastava
  - Kamal Gupta
conference: ICCV2023
citekey: girish2023-shacira
tags:
---
# SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations

**æ–‡ç« é“¾æ¥**: [Zotero](zotero://select/library/items/HSTB42UP) 
**ç½‘é¡µé“¾æ¥**: [URL](https://openaccess.thecvf.com/content/ICCV2023/html/Girish_SHACIRA_Scalable_HAsh-grid_Compression_for_Implicit_Neural_Representations_ICCV_2023_paper.html)
## Abstract

>[!abstract]
>>Recently, learnable feature grids proposed by M Ìˆ uller et al.  have allowed significant speed-up in the training as well as the sampling of INRs by replacing a large neural network with a multi-resolution look-up table of feature vectors and a much smaller neural network.
>
>åœ¨ INR æ–¹æ³•ä¸­ï¼ŒåŸºäºå¯å­¦ä¹ çš„ç‰¹å¾ç½‘æ ¼çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå¤§å¤§åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œå‡å°‘ç½‘ç»œçš„å¤§å°ã€‚
>>However, these feature grids come at the expense of large memory consumption which can be a bottleneck for storage and streaming applications.
>
>åœ¨å¼•å…¥ç‰¹å¾ç½‘æ ¼æ—¶ï¼Œæå¤§åœ°å¢åŠ äº†å†…å­˜è´Ÿæ‹…ã€‚
>>we propose SHACIRA, a simple yet effective task-agnostic framework for compressing such feature grids with no additional posthoc pruning/quantization stages.
>
>æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯å¯¹ [[muller2022-instant|(Thomas MÃ¼ller, 2022)]] çš„æ”¹è¿›ï¼Œå®ƒå¯¹äºç‰¹å¾ç½‘æ ¼è¿›è¡Œäº†è¿›ä¸€æ­¥å‹ç¼©ï¼Œè€Œä¸æŸå¤±æ€§èƒ½ã€‚

## Comments

### å‰ç½®å·¥ä½œ

> [!note] Page 2
> 
> We use an entropy regularization loss on the latent features to reduce the size of the discrete latents without significantly affecting the reconstruction performance.
> 
> ---
> ğŸ”¤æˆ‘ä»¬å¯¹æ½œåœ¨ç‰¹å¾ä½¿ç”¨ç†µæ­£åˆ™åŒ–æŸå¤±æ¥å‡å°ç¦»æ•£æ½œåœ¨ç‰¹å¾çš„å¤§å°ï¼Œè€Œä¸ä¼šæ˜¾ç€å½±å“é‡å»ºæ€§èƒ½ã€‚ğŸ”¤
> ^QCRZFZFKaDVJ9KBESp2

### é»„è‰²

> [!note] Page 2
> 
> Unsurprisingly, several methods have been proposed to compress INRs using quantization [10, 14, 18], pruning, or a combination of both [13]. The focus of these works is to compress the weights of the MLP, which often leads to either a big drop in the reconstruction quality, or slow convergence for high resolution signals.
> 
> ---
> ğŸ”¤æ¯«ä¸å¥‡æ€ªï¼Œå·²ç»æå‡ºäº†å‡ ç§ä½¿ç”¨é‡åŒ–[10,14,18]ã€ä¿®å‰ªæˆ–ä¸¤è€…ç»„åˆ[13]æ¥å‹ç¼©INRçš„æ–¹æ³•ã€‚è¿™äº›å·¥ä½œçš„é‡ç‚¹æ˜¯å‹ç¼© MLP çš„æƒé‡ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´é‡å»ºè´¨é‡å¤§å¹…ä¸‹é™ï¼Œæˆ–è€…é«˜åˆ†è¾¨ç‡ä¿¡å·çš„æ”¶æ•›é€Ÿåº¦å˜æ…¢ã€‚ğŸ”¤
> ^8JVC9VYVaDVJ9KBESp2

> [!note] Page 2
> 
> This shifts the burden of representing the signal to the feature grid instead of the MLP.
> 
> ---
> ğŸ”¤è¿™å°†è¡¨ç¤ºä¿¡å·çš„è´Ÿæ‹…è½¬ç§»åˆ°ç‰¹å¾ç½‘æ ¼è€Œä¸æ˜¯ MLPã€‚ğŸ”¤
> ^QMXD2C32aDVJ9KBESp2

> [!note] Page 2
> 
> However, the size of the feature grids can be very large which is not memory efficient and impractical for many real-world applications with network bandwidth or storage constraints.
> 
> ---
> ğŸ”¤ç„¶è€Œï¼Œç‰¹å¾ç½‘æ ¼çš„å¤§å°å¯èƒ½éå¸¸å¤§ï¼Œè¿™å¯¹äºè®¸å¤šå…·æœ‰ç½‘ç»œå¸¦å®½æˆ–å­˜å‚¨é™åˆ¶çš„å®é™…åº”ç”¨ç¨‹åºæ¥è¯´å†…å­˜æ•ˆç‡ä¸é«˜ä¸”ä¸åˆ‡å®é™…ã€‚ğŸ”¤
> ^9W3JI8Q8aDVJ9KBESp2

> [!note] Page 2
> 
> Our feature grid consists of quantized feature vectors and parameterized decoders which transform the feature vectors into continuous values before passing them to MLP.
> 
> ---
> ğŸ”¤æˆ‘ä»¬çš„ç‰¹å¾ç½‘æ ¼ç”±é‡åŒ–ç‰¹å¾å‘é‡å’Œå‚æ•°åŒ–è§£ç å™¨ç»„æˆï¼Œå‚æ•°åŒ–è§£ç å™¨åœ¨å°†ç‰¹å¾å‘é‡ä¼ é€’ç»™ MLP ä¹‹å‰å°†å…¶è½¬æ¢ä¸ºè¿ç»­å€¼ã€‚ğŸ”¤
> ^FGSY9JMNaDVJ9KBESp2

> [!note] Page 2
> 
> we employ an annealing approach to the discrete latents which improves the training stability of the latents,
> 
> ---
> ğŸ”¤æˆ‘ä»¬å¯¹ç¦»æ•£æ½œåœ¨å˜é‡é‡‡ç”¨é€€ç«æ–¹æ³•ï¼Œæé«˜äº†æ½œåœ¨å˜é‡çš„è®­ç»ƒç¨³å®šæ€§ï¼ŒğŸ”¤
> ^I6MZQT2AaDVJ9KBESp2

> [!note] Page 2
> 
> As seen in Figure 1, the proposed approach is able to compress feature-grid methods such as Instant-NGP [1] with almost an order of magnitude while retaining the performance in terms of PSNR for gigapixel images and 3D scenes from the RTMV dataset [20].
> 
> ---
> ğŸ”¤å¦‚å›¾ 1 æ‰€ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿå°†ç‰¹å¾ç½‘æ ¼æ–¹æ³•ï¼ˆä¾‹å¦‚ Instant-NGP [1]ï¼‰å‹ç¼©å‡ ä¹ä¸€ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶ä¿ç•™æ¥è‡ª RTMV æ•°æ®é›†çš„åäº¿åƒç´ å›¾åƒå’Œ 3D åœºæ™¯çš„ PSNR æ€§èƒ½[ 20]ã€‚ğŸ”¤
> ^NKBUE4E4aDVJ9KBESp2

> [!note] Page 2
> 
> The key contribution of our work is to directly compress the learnable feature grid with proposed entropy regularization loss and highlight its adaptibility to diverse signals.
> 
> ---
> ğŸ”¤æˆ‘ä»¬å·¥ä½œçš„å…³é”®è´¡çŒ®æ˜¯åˆ©ç”¨æå‡ºçš„ç†µæ­£åˆ™åŒ–æŸå¤±ç›´æ¥å‹ç¼©å¯å­¦ä¹ çš„ç‰¹å¾ç½‘æ ¼ï¼Œå¹¶çªå‡ºå…¶å¯¹ä¸åŒä¿¡å·çš„é€‚åº”æ€§ã€‚ğŸ”¤
> ^FXBUCKBCaDVJ9KBESp2

> [!note] Page 2
> 
> highlight the effectiveness of our approach to compress feature-grid based INRs and its advantages over MLP-based INRs.
> 
> ---
> ğŸ”¤å¼ºè°ƒæˆ‘ä»¬å‹ç¼©åŸºäºç‰¹å¾ç½‘æ ¼çš„ INR çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§åŠå…¶ç›¸å¯¹äºåŸºäº MLP çš„ INR çš„ä¼˜åŠ¿ã€‚ğŸ”¤
> ^3KGLCFIMaDVJ9KBESp2

> [!note] Page 4
> 
> While feature-grid INRs can converge faster than pureMLP approaches, the space required to store all the parameters at different levels can rise very rapidly if we want highfidelity reconstructions for high-resolution inputs.
> 
> ---
> ğŸ”¤è™½ç„¶ç‰¹å¾ç½‘æ ¼ INR å¯ä»¥æ¯” pureMLP æ–¹æ³•æ”¶æ•›å¾—æ›´å¿«ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³è¦å¯¹é«˜åˆ†è¾¨ç‡è¾“å…¥è¿›è¡Œé«˜ä¿çœŸåº¦é‡å»ºï¼Œåˆ™å­˜å‚¨ä¸åŒçº§åˆ«çš„æ‰€æœ‰å‚æ•°æ‰€éœ€çš„ç©ºé—´å¯èƒ½ä¼šè¿…é€Ÿå¢åŠ ã€‚ğŸ”¤
> ^6D7DTIP2aDVJ9KBESp4

> [!note] Page 4
> 
> we propose a parameterized decoder dÎ¸
> 
> ---
> ğŸ”¤æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå‚æ•°åŒ–è§£ç å™¨ dÎ¸ğŸ”¤
> ^MBXXPXCTaDVJ9KBESp4

> [!note] Page 4
> 
> we observed that a single shared decoder parameterized as a linear transform across all L levels works pretty well
> 
> ---
> ğŸ”¤æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå‚æ•°åŒ–ä¸ºè·¨æ‰€æœ‰ L å±‚çš„çº¿æ€§å˜æ¢çš„å•ä¸ªå…±äº«è§£ç å™¨æ•ˆæœå¾ˆå¥½ğŸ”¤
> ^89F3IHKPaDVJ9KBESp4

> [!note] Page 4
> 
> utilize the Straight-Through Estimator [48](STE.md).
> 
> ---
> ğŸ”¤åˆ©ç”¨ç›´é€šä¼°è®¡å™¨[48]ï¼ˆSTEï¼‰ã€‚ğŸ”¤
> ^I232WE77aDVJ9KBESp4

> [!note] Page 4
> 
> The gradients are propagated through the samples using the Gumbel reparameterization trick
> 
> ---
> ğŸ”¤ä½¿ç”¨ Gumbel é‡æ–°å‚æ•°åŒ–æŠ€å·§åœ¨æ ·æœ¬ä¸­ä¼ æ’­æ¢¯åº¦ğŸ”¤
> ^HQF7PFXRaDVJ9KBESp4

> [!note] Page 5
> 
> we optimize the parameters Ï† of MLP gÏ†, discrete feature grid b Q, discrete to continuous decoder Î¸, and the probability models {Pd}
> 
> ---
> ğŸ”¤æˆ‘ä»¬ä¼˜åŒ– MLP gÏ† çš„å‚æ•° Ï†ã€ç¦»æ•£ç‰¹å¾ç½‘æ ¼ b Qã€ç¦»æ•£åˆ°è¿ç»­è§£ç å™¨ Î¸ å’Œæ¦‚ç‡æ¨¡å‹ {Pd}ğŸ”¤
> ^AZFUB9ZCaDVJ9KBESp5

> [!note] Page 5
> 
> Kodak dataset
> 
> ---
> ğŸ”¤æŸ¯è¾¾æ•°æ®é›†ğŸ”¤
> ^TLTGHLFVaDVJ9KBESp5

> [!note] Page 5
> 
> varying from 2M pixels to 1G pixels
> 
> ---
> ğŸ”¤ä»2Måƒç´ åˆ°1Gåƒç´ ä¸ç­‰ğŸ”¤
> ^UCK4QS27aDVJ9KBESp5

> [!note] Page 5
> 
> RTMV dataset
> 
> ---
> ğŸ”¤RTMV æ•°æ®é›†ğŸ”¤
> ^6KSV6FIYaDVJ9KBESp5

> [!note] Page 5
> 
> UVG dataset
> 
> ---
> ğŸ”¤UVGæ•°æ®é›†ğŸ”¤
> ^VHD7LZCKaDVJ9KBESp5






## Introduction

é’ˆå¯¹ INR æ¨¡å‹çš„å‹ç¼©æ–¹æ³•ä¸å±€é™æ€§ï¼š
1. å¯¹ç½‘ç»œçš„æƒé‡è¿›è¡Œå‹ç¼©ã€‚è¿™å®¹æ˜“å¯¼è‡´é‡å»ºè´¨é‡çš„ä¸‹é™ã€‚
2. è®¾ç½®ä¸€ç³»åˆ—ä¸åŒåˆ†è¾¨ç‡çš„å¯å­¦ä¹ çš„ feature grid, å°†ç½‘ç»œçš„è®­ç»ƒè´Ÿæ‹…è½¬ç§»ä¸€éƒ¨åˆ†åˆ° feature grid ä¸Šï¼Œèƒ½å¤Ÿå‡å°ç½‘ç»œçš„å¤§å°ã€‚ä½†å­˜å‚¨ feature grid éœ€è¦å·¨å¤§å¸¦å®½ï¼Œé™åˆ¶äº†åœ¨ä¸€äº›æ–¹é¢çš„åº”ç”¨ã€‚

æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ¦‚è¦ï¼š
1. faeture grid ä¸ç›´æ¥å­˜å‚¨ï¼Œè€Œæ˜¯ç”±æ›´å°çš„ feature vector ç»è¿‡è§£ç å™¨ç”Ÿæˆã€‚
2. ä½¿ç”¨ç†µæ¨¡å‹å¯¹ feature vector è¿›è¡Œä¼°è®¡ï¼Œå‡å°å­˜å‚¨ç©ºé—´ã€‚
3. åœ¨ feature vector çš„é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œè®¾è®¡äº†ä¸€ç§é€€ç«æ–¹æ³•ï¼Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

feature vectorï¼Œç½‘ç»œå‚æ•°ï¼Œè§£ç å™¨å‚æ•°ï¼Œç†µæ¨¡å‹å¯ä»¥ä¸€èµ·è®­ç»ƒï¼Œæ˜¯ç«¯åˆ°ç«¯çš„å¯å­¦ä¹ æ¡†æ¶ã€‚

## Approach

### Feature-grid INRs

æµæ°´çº¿çš„æœ€åä¸€éƒ¨åˆ†ï¼Œæ­¤éƒ¨åˆ†å³æ˜¯ [[muller2022-instant|(Thomas MÃ¼ller, 2022)]] æå‡ºçš„æ–¹æ³•ã€‚


![[Pasted image 20231210113748.png]]

å®šä¹‰ï¼š
- feature grid ç”± $\mathbf{Z}=\{\mathbf{Z}_1,\ldots,\mathbf{Z}_l\}$ è¡¨ç¤ºï¼Œ$\mathbf Z_i$ è¡¨ç¤ºä¸åŒåˆ†è¾¨ç‡å¯¹åº”çš„çŸ©é˜µã€‚
- $\mathbf{Z}_l\in\mathbb{R}^{T_l\times F},\mathrm{~where~}T_l=\min(R_l^2,T)$ï¼Œè®¾ç½®å“ˆå¸Œè¡¨æ¡ç›®ä¸Šé™ Tï¼Œç‰¹å¾é•¿åº¦ $F$ å›ºå®šã€‚


$$
\begin{align*}
\mathbf{f}_l(\mathbf{x})&=\text{interp}(\mathbf{Z}_l[tl],\mathbf{Z}_l[tr],\mathbf{Z}_l[bl],\mathbf{Z}_l[br])\\
\widehat{\mathbf{y}}&=g_\phi(\mathrm{concat}[\mathbf{f}_1(\mathbf{x}),\ldots,\mathbf{f}_L(\mathbf{x})])
\end{align*}
$$
### Feature-grid reparameterization

![[Pasted image 20231210115018.png]]

å¯¹äºæ¯ä¸ªçŸ©é˜µ $\mathbf Z_{l}\in\mathbb{R}^{T_{l} \times F}$ ï¼Œç”±å¯¹åº”çš„ $\mathbf{Q}_l\in\mathbb{R}^{T_l\times D}$ æ¥å‹ç¼©ï¼Œç”±è§£ç å™¨æ¥ $d_\theta:\mathbb{R}^D\to\mathbb{R}^F$ è½¬æ¢ï¼Œå…¨éƒ¨è§£ç å™¨å…±äº«å‚æ•°ã€‚
å°½ç®¡å¯ä»¥å¯¹ä¸åŒåˆ†è¾¨ç‡å±‚ä½¿ç”¨ä¸åŒçš„è§£ç å™¨ï¼Œä½œè€…è§‚å¯Ÿå¾—å‡ºç»“è®ºï¼šå…±äº«å‚æ•°çš„çº¿æ€§è½¬æ¢è§£ç å™¨åœ¨å¹³è¡¡è®­ç»ƒæ¨ç†æ—¶é—´å’Œé‡å»ºè´¨é‡ä¸Šæœ€å¥½ã€‚

$\mathbf Q$ ç”± $\widehat{\mathbf{Q}}$ é‡åŒ–å¾—åˆ°ï¼ˆround å¯»æ‰¾æœ€è¿‘çš„æ•´æ•°ï¼‰ï¼Œæ­¤è¿‡ç¨‹ä¸å¯å¾®ï¼Œä½œè€…æå‡ºä½¿ç”¨ [[STE]] æ¥è§£å†³æ­¤é—®é¢˜ã€‚<mark style="background: #FF5582A6;">ä½†æ˜¯ STE ä¸­ç®€å•çš„è¿‘ä¼¼ä¼šå¸¦æ¥è¾ƒå¤§çš„èˆå…¥è¯¯å·®</mark>ï¼š$\|\mathbf{Q}-\widehat{\mathbf{Q}}\|$ ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ª soft rounding operationã€‚

ä»¥ä¸€å®šçš„æ¦‚ç‡æ¥é€‰æ‹© $\lfloor\widehat{\mathbf{Q}}\rfloor$ å’Œ $\lceil\widehat{\mathbf{Q}}\rceil$ï¼Œä½¿ç”¨ä¸€ä¸ª one-hot ç¼–ç çš„å‘é‡ $b$ï¼š
$$\mathbf{Q}=b\lfloor\widehat{\mathbf{Q}}\rfloor+(1-b)\lceil\widehat{\mathbf{Q}}\rceil.$$
å…¶ä¸­ $b$ å– $0ï¼Œ1$ çš„æ¦‚ç‡åˆ†åˆ«æ˜¯ï¼š
$$\begin{aligned}&\mathrm{Prob}(b=0)\propto\exp\left\{-\tanh^{-1}\left(\widehat{\mathbf{Q}}-\lfloor\widehat{\mathbf{Q}}\rfloor\right)/\tau\right\}\\&\mathrm{Prob}(b=1)\propto\exp\left\{-\tanh^{-1}\left(\lceil\widehat{\mathbf{Q}}\rceil-\widehat{\mathbf{Q}}\right)/\tau\right\}\end{aligned}$$
ä½¿ç”¨ [[Gumbel Softmax]] æ¥ç”Ÿæˆ one-hot å‘é‡ b.
åœ¨è®­ç»ƒçš„åˆæœŸï¼Œ$\tau$ æ¯”è¾ƒå¤§ï¼ˆæ¥è¿‘ 1ï¼‰ï¼Œæ­¤è½¯åŒ–æ“ä½œå¹¶ä¸èƒ½å¾ˆå¥½çš„è¿‘ä¼¼ round å‡½æ•°ï¼Œä½†èƒ½æä¾›å¾ˆå¥½çš„æ¢¯åº¦ä¼ é€’ï¼Œåœ¨è®­ç»ƒåæœŸï¼Œ$\tau$ è¶‹äº 0ï¼Œæ­¤è½¯åŒ–æ“ä½œæ— é™æ¥è¿‘ round å‡½æ•°ã€‚

å¯¹æ­¤æ“ä½œæ‰€åšçš„æ¶ˆèå®éªŒï¼š
ä»¥æ­¤è½¯åŒ–æ“ä½œçš„æŒç»­æ—¶é—´ä¸ºå˜é‡ä½œå›¾
![[Pasted image 20231210134639.png]]

### Feature-grid compression
éšç©ºé—´æœ‰ $D$ ç»´ï¼Œä¸ºæ¯ä¸€ç»´å¼•å…¥ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼š$P_d:\mathbb{R}\rightarrow[0,1]$ã€‚
$$
\begin{align*}
 \mathcal{L}_I (\widehat{\mathbf{Q}})&=-\frac 1 T\sum_{d=1}^D\sum_{i=1}^T\log_2\left (P_d\left (\widehat{\mathbf{Q}}[i, d]+n\right)\right)\\
where \ &n \sim \mathcal{U}[-1,1] 
\end{align*}
$$
æ¶ˆèå®éªŒï¼š
![[Pasted image 20231210140606.png]]
### End-to-end optimization
æ€»ä½“æµç¨‹ï¼š
$$
\begin{aligned}\mathbf{Q}&=\mathrm{discretize}(\widehat{\mathbf{Q}})\\\mathbf{Z}&=d_\theta(\mathbf{Q})\\\mathbf{\widehat{y}}&=g_\phi\left(\mathrm{concat}\left[\mathrm{interp}(\mathbf{Z},\mathbf{x})\right]\right)\end{aligned}
$$
ä½¿ç”¨æŸå¤±å‡½æ•°ï¼š
$$
\mathcal{L}_{\mathrm{MSE}}(\widehat{\mathbf{y}},\mathbf{y})+\lambda_I\mathcal{L}_I(\widehat{\mathbf{Q}})
$$
æ¥è®­ç»ƒå…¨éƒ¨å‚æ•°ã€‚æœ€åä½¿ç”¨ç®—æœ¯ç¼–ç ï¼Œå– $P_d$ çš„æ¦‚ç‡ï¼Œæ¥æ— æŸç¼–ç  $Q$ã€‚

## Experiments
åœ¨ Kodak æ•°æ®é›†ä¸Šçš„ç»“æœï¼š
![[Pasted image 20231210140113.png]]

å¯¹ä¸åŒåˆ†è¾¨ç‡å›¾ç‰‡çš„ç»“æœï¼š
![[Pasted image 20231210140307.png|500]]
æ”¶æ•›é€Ÿåº¦å¯¹æ¯”ï¼š
![[Pasted image 20231210140730.png]]