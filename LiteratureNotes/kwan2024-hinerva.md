---
zotero-key: ELZIKZKE
zt-attachments:
  - "290"
title: "HiNeRV: Video Compression with Hierarchical Encoding-based Neural Representation"
authors:
  - Ho Man Kwan
  - Ge Gao
  - Fan Zhang
  - Andrew Gower
  - David Bull
doi: 10.5555/3666122.3669299
conference: xxx
citekey: kwan2024-hinerva
tags: []
---
# HiNeRV: Video Compression with Hierarchical Encoding-based Neural Representation

**æ–‡ç« é“¾æŽ¥**: [Zotero](zotero://select/library/items/ELZIKZKE) [attachment](<file:///home/ilot/Zotero/storage/NXTAQGZ4/Kwan%20%E7%AD%89%20-%202024%20-%20HiNeRV%20Video%20Compression%20with%20Hierarchical%20Encodi.pdf>)
**ç½‘é¡µé“¾æŽ¥**: [URL](http://arxiv.org/abs/2306.09818)
## Abstract

>[!abstract]
>Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).

## Comments

### å…ˆå‰ç ”ç©¶çš„é—®é¢˜

> [!note] Page 1
> 
> However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression.
> 
> ---
> ðŸ”¤ç„¶è€Œï¼ŒçŽ°æœ‰çš„åŸºäºŽ INR çš„æ–¹æ³•æœªèƒ½æä¾›ä¸Žè§†é¢‘åŽ‹ç¼©é¢†åŸŸçš„æœ€æ–°æŠ€æœ¯ç›¸å½“çš„é€ŸçŽ‡è´¨é‡æ€§èƒ½ã€‚ðŸ”¤
> ^ZDU6Y7D6aNXTAQGZ4p1

### ä»£ç ”ç©¶ç‚¹

> [!note] Page 1
> 
> lity and speed [12]. To address this limitation, recent works have employed Convolutional Neural Networks
> 
> ---
> ðŸ”¤å¼ºåº¦å’Œé€Ÿåº¦[12]ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæœ€è¿‘çš„å·¥ä½œé‡‡ç”¨äº†å·ç§¯ç¥žç»ç½‘ç»œðŸ”¤
> ^88RZQTGYaNXTAQGZ4p1

### åˆ›æ–°ç‚¹

> [!note] Page 1
> 
> mploys depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on H
> 
> ---
> ðŸ”¤é‡‡ç”¨æ·±åº¦å·ç§¯ã€MLP å’Œæ’å€¼å±‚æ¥æž„å»ºå…·æœ‰é«˜å®¹é‡çš„æ·±è€Œå®½çš„ç½‘ç»œæž¶æž„ã€‚ HiNeRV ä¹Ÿæ˜¯ä¸€ç§åŒæ—¶å¯¹å¸§å’Œè¡¥ä¸ä¸­çš„è§†é¢‘è¿›è¡Œç»Ÿä¸€è¡¨ç¤ºç¼–ç çš„æ–¹æ³•ï¼Œå®ƒæ¯”çŽ°æœ‰æ–¹æ³•æä¾›äº†æ›´é«˜çš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æž„å»ºäº†åŸºäºŽHçš„è§†é¢‘ç¼–è§£ç å™¨ðŸ”¤
> ^5LYUFALNaNXTAQGZ4p1

### ç–‘æƒ‘ç‚¹

> [!note] Page 1
> 
> index to video frame mapping [12, 31, 6, 27, 11]. These CNN-based INRs are capable of reconstructing video content with higher quality and with a faster decoding speed, when compared to MLP-based appro
> 
> ---
> ðŸ”¤è§†é¢‘å¸§æ˜ å°„çš„ç´¢å¼•[12,31,6,27,11]ã€‚ä¸ŽåŸºäºŽ MLP çš„åº”ç”¨ç¨‹åºç›¸æ¯”ï¼Œè¿™äº›åŸºäºŽ CNN çš„ INR èƒ½å¤Ÿä»¥æ›´é«˜çš„è´¨é‡å’Œæ›´å¿«çš„è§£ç é€Ÿåº¦é‡å»ºè§†é¢‘å†…å®¹ã€‚ðŸ”¤
> ^DF3FEUDKaNXTAQGZ4p1

### æ€§èƒ½æˆç»©

> [!note] Page 1
> 
> ion. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall
> 
> ---
> ðŸ”¤ç¦»å­ã€‚æ‰€æå‡ºçš„æ–¹æ³•å·²åœ¨ç”¨äºŽè§†é¢‘åŽ‹ç¼©çš„ UVG å’Œ MCL-JCV æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ŽåŸºäºŽå­¦ä¹ çš„ç¼–è§£ç å™¨ï¼ˆæ€»ä½“ 72.3ï¼…ï¼‰ç›¸æ¯”ï¼Œè¯æ˜Žäº†æ‰€æœ‰çŽ°æœ‰ INR åŸºçº¿å’Œç«žäº‰æ€§èƒ½çš„æ˜¾ç€æ”¹è¿›ðŸ”¤
> ^S89BMHUMaNXTAQGZ4p1

### æŠ€æœ¯ç»†èŠ‚

> [!note] Page 1
> 
> ] and videos [50, 12]. INRs typically learn a coordinate to value mapping (e.g. mapping a pixel or voxel index to its color and/or occupancy) to support implicit reconstruction of the original signal. While the
> 
> ---
> ðŸ”¤] å’Œè§†é¢‘ [50, 12]ã€‚ INR é€šå¸¸å­¦ä¹ åæ ‡åˆ°å€¼çš„æ˜ å°„ï¼ˆä¾‹å¦‚ï¼Œå°†åƒç´ æˆ–ä½“ç´ ç´¢å¼•æ˜ å°„åˆ°å…¶é¢œè‰²å’Œ/æˆ–å ç”¨ï¼‰ä»¥æ”¯æŒåŽŸå§‹ä¿¡å·çš„éšå¼é‡å»ºã€‚è™½ç„¶ðŸ”¤
> ^JTKSTWA8aNXTAQGZ4p1

