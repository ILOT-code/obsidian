---
zotero-key: ARKSB7UP
zt-attachments:
  - "277"
title: Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate Distortion Optimization
authors:
  - Zhiyu Zhang
  - Guo Lu
  - Huanxiong Liang
  - Anni Tang
  - Qiang Hu
  - Li Song
doi: 10.48550/arXiv.2402.01380
conference: xxx
citekey: zhang2024-efficient
tags: []
---
# Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate Distortion Optimization

**æ–‡ç« é“¾æŽ¥**: [Zotero](zotero://select/library/items/ARKSB7UP) [attachment](<file:///home/ilot/Zotero/storage/6UZRAARJ/Zhang%20%E7%AD%89%20-%202024%20-%20Efficient%20Dynamic-NeRF%20Based%20Volumetric%20Video%20Codi.pdf>)
**ç½‘é¡µé“¾æŽ¥**: [URL](http://arxiv.org/abs/2402.01380)
## Abstract

æœ¬æ–‡ä½¿ç”¨åŠ¨æ€çš„ç¥žç»è¾å°„åœºæŠ€æœ¯æ¥åŽ‹ç¼©ä½“ç´ è§†é¢‘ã€‚
å…¶å·¥ä½œæžå¤§ç¨‹åº¦å»ºç«‹åœ¨ [[chen2023-factor|(Anpei Chen, 2023)]] ä¸Šã€‚ä½œè€…å°†å®ƒæ‰©å±•åˆ°æ—¶é—´ç»´åº¦ï¼Œå¹¶å¼•å…¥äº† rate-estimate å’ŒåŠ å™ªå£°æ¨¡æ‹Ÿé‡åŒ–çš„æŠ€æœ¯ï¼Œå®žçŽ°äº† end-to-end çš„è®­ç»ƒã€‚

## Intro
ä½œè€…åˆ†æžäº†å…ˆå‰çš„ SOTA æŠ€æœ¯ ReRF çš„ä¸è¶³ï¼š
1. åœ¨ 3 D å±‚é¢ï¼Œè¿›è¡Œè¿åŠ¨ä¼°è®¡æ˜¯å¾ˆå›°éš¾çš„äº‹ï¼ŒReRF ä¸­çš„è¿åŠ¨ä¼°è®¡ä¹Ÿåªä½¿ç”¨äº†å¾ˆå°‘çš„å‚æ•°ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å¸§é—´ä¿¡æ¯ã€‚
2. å®ƒçš„æ¨¡åž‹è®­ç»ƒä»¥åŠä¹‹åŽçš„æ¨¡åž‹åŽ‹ç¼©è¿‡ç¨‹æ˜¯åˆ†ç¦»çš„ï¼Œæ— æ³•åšåˆ°ç«¯åˆ°ç«¯ã€‚

## Method

$$
(\mathbf{c},\sigma)=g_{\phi}(\mathbf{x},\mathbf{d})
$$
$$
\begin{align*}
\hat{C}(\mathbf{r})&=\sum _{i=1}^{N}T_{i}\alpha_{i}\mathbf{c}_{i}\\
T_{i}&=\prod_{j=1}^{i-1}(1-\alpha _{i}), ~~\alpha_{i}=1-\exp(-\sigma_{i}\delta_{i})
\end{align*}
$$

$T_{i}$ è¡¨ç¤ºé€è¿‡çŽ‡ï¼Œ$\alpha$ è¡¨ç¤ºé€æ˜Žåº¦ã€‚

$$
\begin{aligned}
\mathbf{c(x)}& =\text{interp}\left(\mathbf{x},\mathbf{C}\right) \\
\mathbf{b(x)}& =\text{interp}\left(\gamma(\mathbf{x}),\mathbf{B}\right) \\
(\mathbf{c},\sigma)& =\mathcal{P}(\mathbf{c}(\mathbf{x})\circ\mathbf{b}(\mathbf{x}),\mathbf{d}) 
\end{aligned}
$$
## Comments

### å…ˆå‰ç ”ç©¶çš„é—®é¢˜

> [!note] Page 1
> 
> However, ReRF separates the modeling from compression process, resulting in suboptimal compression efficiency.
> 
> ---
> ðŸ”¤ç„¶è€Œï¼ŒReRF å°†å»ºæ¨¡ä¸ŽåŽ‹ç¼©è¿‡ç¨‹åˆ†å¼€ï¼Œå¯¼è‡´åŽ‹ç¼©æ•ˆçŽ‡ä¸ç†æƒ³ã€‚ðŸ”¤
> ^8K4DP2TZa6UZRAARJp1

> [!note] Page 1
> 
> However, the acquisition of volumetric video typically requires multiple cameras capturing from different angles, resulting in data volumes that can be several times larger than traditional 2D videos [2], which poses significant challenges in terms of storage and transmission.
> 
> ---
> ðŸ”¤ç„¶è€Œï¼Œä½“è§†é¢‘çš„é‡‡é›†é€šå¸¸éœ€è¦å¤šä¸ªæ‘„åƒæœºä»Žä¸åŒè§’åº¦è¿›è¡Œé‡‡é›†ï¼Œå¯¼è‡´æ•°æ®é‡å¯èƒ½æ¯”ä¼ ç»Ÿ2Dè§†é¢‘å¤§æ•°å€[2]ï¼Œè¿™ç»™å­˜å‚¨å’Œä¼ è¾“å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ðŸ”¤
> ^8P43ZWXFa6UZRAARJp1

> [!note] Page 1
> 
> While geometry-based solutions [4] involve the reconstruction and compression of dynamic point clouds, but they are vulnerable to occlusions and textureless regions.
> 
> ---
> ðŸ”¤è™½ç„¶åŸºäºŽå‡ ä½•çš„è§£å†³æ–¹æ¡ˆ[4]æ¶‰åŠåŠ¨æ€ç‚¹äº‘çš„é‡å»ºå’ŒåŽ‹ç¼©ï¼Œä½†å®ƒä»¬å¾ˆå®¹æ˜“å—åˆ°é®æŒ¡å’Œæ— çº¹ç†åŒºåŸŸçš„å½±å“ã€‚ðŸ”¤
> ^XBVTRXX8a6UZRAARJp1

> [!note] Page 2
> 
> However, accurately estimating the 3D motion grid proves to be a challenging task,
> 
> ---
> ðŸ”¤ç„¶è€Œï¼Œå‡†ç¡®ä¼°è®¡ 3D è¿åŠ¨ç½‘æ ¼è¢«è¯æ˜Žæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ŒðŸ”¤
> ^ZHIA48STa6UZRAARJp2

> [!note] Page 2
> 
> Moreover, the modeling and compression process of the dynamic radiance field in ReRF are separate.
> 
> ---
> ðŸ”¤æ­¤å¤–ï¼ŒReRFä¸­åŠ¨æ€è¾å°„åœºçš„å»ºæ¨¡å’ŒåŽ‹ç¼©è¿‡ç¨‹æ˜¯åˆ†å¼€çš„ã€‚ðŸ”¤
> ^3A8M8FTDa6UZRAARJp2

> [!note] Page 3
> 
> However, such a method not only neglects the temporal continuity but also poses challenges for compression.
> 
> ---
> ðŸ”¤ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…å¿½ç•¥äº†æ—¶é—´è¿žç»­æ€§ï¼Œè€Œä¸”ç»™åŽ‹ç¼©å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ðŸ”¤
> ^M5HHE6ZKa6UZRAARJp3

### åˆ›æ–°ç‚¹

> [!note] Page 1
> 
> Specifically, we decompose the NeRF representation into the coefficient fields and the basis fields, incrementally updating the basis fields in the temporal domain to achieve dynamic modeling. Additionally, we perform end-to-end joint optimization on the modeling and compression process to further improve the compression efficiency.
> 
> ---
> ðŸ”¤å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°† NeRF è¡¨ç¤ºåˆ†è§£ä¸ºç³»æ•°åŸŸå’ŒåŸºåŸŸï¼Œåœ¨æ—¶åŸŸä¸­å¢žé‡æ›´æ–°åŸºåŸŸä»¥å®žçŽ°åŠ¨æ€å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å»ºæ¨¡å’ŒåŽ‹ç¼©è¿‡ç¨‹è¿›è¡Œç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜åŽ‹ç¼©æ•ˆçŽ‡ã€‚ðŸ”¤
> ^JXVQMGNZa6UZRAARJp1

### æŠ€æœ¯ç»†èŠ‚

> [!note] Page 2
> 
> To accelerate training and rendering, DiF [9] decomposes the representation of NeRF into the coefficient fields and the basis fields. The basis fields capture the commonality of the signal, and the coefficient fields represent the spatial variation of the signal. Specifically, the coefficient fields are represented by a single-scale 3D grid C, while the basis fields are represented by multi-scale 3D grids B.
> 
> ---
> ðŸ”¤ä¸ºäº†åŠ é€Ÿè®­ç»ƒå’Œæ¸²æŸ“ï¼ŒDiF [9] å°† NeRF çš„è¡¨ç¤ºåˆ†è§£ä¸ºç³»æ•°åŸŸå’ŒåŸºåŸŸã€‚åŸºåœºæ•èŽ·ä¿¡å·çš„å…±æ€§ï¼Œç³»æ•°åœºè¡¨ç¤ºä¿¡å·çš„ç©ºé—´å˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œç³»æ•°åœºç”±å•å°ºåº¦3Dç½‘æ ¼Cè¡¨ç¤ºï¼Œè€ŒåŸºåœºç”±å¤šå°ºåº¦3Dç½‘æ ¼Bè¡¨ç¤ºã€‚ðŸ”¤
> ^SIXKVF9Ta6UZRAARJp2

> [!note] Page 3
> 
> During training, we only update the coefficient fields Ct and the
> 
> ---
> ðŸ”¤åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬åªæ›´æ–°ç³»æ•°åŸŸ Ct å’ŒðŸ”¤
> ^5CFA6U2Aa6UZRAARJp3

> [!note] Page 3
> 
> residual fields Rt for the current frame.
> 
> ---
> ðŸ”¤å½“å‰å¸§çš„å‰©ä½™å­—æ®µ Rtã€‚ðŸ”¤
> ^URFKITATa6UZRAARJp3

> [!note] Page 3
> 
> Addtionally, to ensure temporal continuity and facilitate compression, we apply L1 regularization to the residual fields:
> 
> ---
> ðŸ”¤æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿æ—¶é—´è¿žç»­æ€§å¹¶ä¿ƒè¿›åŽ‹ç¼©ï¼Œæˆ‘ä»¬å¯¹æ®‹å·®åœºåº”ç”¨ L1 æ­£åˆ™åŒ–ï¼šðŸ”¤
> ^99YS33D2a6UZRAARJp3

### ç´«è‰²

> [!note] Page 4
> 
> In this paper, we conducted experiments on the ReRF dataset [15] and the Dna-rendering dataset [17].
> 
> ---
> ðŸ”¤åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨ ReRF æ•°æ®é›† [15] å’Œ Dna-rendering æ•°æ®é›† [17] ä¸Šè¿›è¡Œäº†å®žéªŒã€‚ðŸ”¤
> ^IMTFAPTEa6UZRAARJp4

### å“çº¢

> [!note] Page 5
> 
> Baseline. We consider DiFDiF [9] as the baseline for our method. However, while DiF is designed for static scenes, when extending it to dynamic scenes, we employ DiF to model each time step individually. Additionally, to ensure a fair comparison, we utilize the 7zip which is a lossless compression algorithm to compress the DiF models.
> 
> ---
> ðŸ”¤åŸºçº¿ã€‚æˆ‘ä»¬å°† DiFDiF [9] ä½œä¸ºæˆ‘ä»¬æ–¹æ³•çš„åŸºå‡†ã€‚ç„¶è€Œï¼Œè™½ç„¶ DiF æ˜¯ä¸ºé™æ€åœºæ™¯è®¾è®¡çš„ï¼Œä½†å½“å°†å…¶æ‰©å±•åˆ°åŠ¨æ€åœºæ™¯æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ DiF å•ç‹¬å»ºæ¨¡æ¯ä¸ªæ—¶é—´æ­¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨æ— æŸåŽ‹ç¼©ç®—æ³•7zipæ¥åŽ‹ç¼©DiFæ¨¡åž‹ã€‚ðŸ”¤
> ^9L6M9Z36a6UZRAARJp5

