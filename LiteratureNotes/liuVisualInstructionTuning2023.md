---
zotero-key: QTHUVXCX
zt-attachments:
  - "637"
title: Visual Instruction Tuning
authors:
  - Haotian Liu
  - Chunyuan Li
  - Qingyang Wu
  - Yong Jae Lee
doi: 10.48550/arXiv.2304.08485
conference: xxx
citekey: liuVisualInstructionTuning2023
tags: []
---
# Visual Instruction Tuning

**文章链接**: [Zotero](zotero://select/library/items/QTHUVXCX) [attachment](<file:///home/ilot/Documents/Zotero/storage/VA2JBC42/Liu%20%E7%AD%89%20-%202023%20-%20Visual%20Instruction%20Tuning.pdf>)
**网页链接**: [URL](http://arxiv.org/abs/2304.08485)
## Abstract

>[!abstract]
>Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.

## Comments

